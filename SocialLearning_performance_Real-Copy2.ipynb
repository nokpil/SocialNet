{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad368c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader # (testset, batch_size=4,shuffle=False, num_workers=4)\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau as RLRP\n",
    "from torch.nn.parallel import DistributedDataParallel, DataParallel\n",
    "from torch.nn.init import xavier_normal\n",
    "from torch.nn.parameter import Parameter\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import importlib\n",
    "import time\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nkmodel as nk\n",
    "import ppo.core as core\n",
    "from ppo.ppo import PPOBuffer\n",
    "from utils.utils import max_mean_clustering_network\n",
    "import envs\n",
    "import json\n",
    "from itertools import product\n",
    "from functools import reduce  \n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52c4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./result/OLP_updated.pickle', 'rb') as f:\n",
    "    real_network = pickle.load(f)\n",
    "\n",
    "candidate = {}\n",
    "max_node_threshold = 1200\n",
    "\n",
    "for network_index in (real_network.network_index):\n",
    "    graph = nx.Graph()\n",
    "    graph.add_edges_from([tuple(x) for x in real_network[real_network['network_index']==network_index]['edges_id'].values[0]]) # add weights to the edges\n",
    "    graph2 = nx.k_core(graph, k=3)\n",
    "    if graph2.number_of_nodes() > 0:\n",
    "        if nx.is_connected(graph2) and graph2.number_of_nodes()/graph.number_of_nodes() > 0.95:\n",
    "            candidate[network_index] = graph2.number_of_nodes()\n",
    "        \n",
    "network_data = real_network[np.isin(real_network['network_index'], list(candidate.keys()))]\n",
    "network_filter = np.logical_and(network_data['networkDomain'] == 'Social', network_data['number_nodes'].values < max_node_threshold )\n",
    "network_data = network_data[network_filter]\n",
    "network_index = network_data.network_index.values\n",
    "network_nodes = [candidate[i] for i in network_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OLP_selected.pickle', 'wb') as f:\n",
    "    pickle.dump(real_network, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a1f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(exp_name, epoch):\n",
    "\n",
    "    #rel_path = f'data/runs/ds_complete_indv_raw_random_SIR_N10K3NN3_new_rand/{exp_name}/{exp_name}_s42/'\n",
    "    rel_path = f'data/runs/{exp_name}/{exp_name}_s42/'\n",
    "\n",
    "    with open(rel_path + \"config.json\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    env_kwargs = json_data['env_kwargs']\n",
    "    env_name = json_data['env_name']\n",
    "    env_kwargs['graph'] = nx.complete_graph\n",
    "    ac_kwargs = json_data['ac_kwargs']\n",
    "    ac_kwargs['activation'] = nn.Tanh()\n",
    "    arch = json_data['arch']\n",
    "    trj_len = json_data['trj_len']\n",
    "    gamma = json_data['gamma']\n",
    "    lam = json_data['lam']\n",
    "    epochs = json_data['epochs']\n",
    "    seed = json_data['seed']\n",
    "    ensemble_num = env_kwargs['E']\n",
    "    agent_num = env_kwargs['M']\n",
    "    env_scheduler_kwargs = {\n",
    "            'local_rank': 0,\n",
    "            'exp_name': exp_name,\n",
    "            'E': env_kwargs['E'],\n",
    "            'N': env_kwargs['N'],\n",
    "            'K': env_kwargs['K'],\n",
    "            'exp': env_kwargs['exp'],\n",
    "            'NGPU': 1, #'data_dir': 'D:\\\\OneDrive\\\\연구\\\\ML\\\\MyProject\\\\SocialNet\\\\SocialNet\\\\data\\\\runs\\\\ds_complete_indv_raw_random_SIR_N10K3NN3_new_rand'\n",
    "        'data_dir': 'D:\\\\OneDrive\\\\연구\\\\ML\\\\MyProject\\\\SocialNet\\\\SocialNet\\\\data\\\\runs'\n",
    "    }\n",
    "    env_kwargs['env_scheduler'] = envs.__dict__['random_env_scheduler'](**env_scheduler_kwargs)\n",
    "    json_data['corr_type'] = 'TT'\n",
    "    env_kwargs['corr_type'] = 'TT'\n",
    "    if len(env_kwargs['reward_type']) < 9:\n",
    "        print('modify')\n",
    "        env_kwargs['reward_type'] = env_kwargs['reward_type'] + '_full'\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env = envs.__dict__[env_name](**env_kwargs)\n",
    "    action_type = env_kwargs['action_type']\n",
    "    extra_type = env_kwargs['extra_type']\n",
    "    extra_num = len(extra_type)\n",
    "    # Instantiate environment\n",
    "    if action_type == 'total':\n",
    "        obs_dim = (env.neighbor_num + 1, env.N + extra_num)  # (3+1, 15+2)\n",
    "        act_dim = env.action_space.n\n",
    "        dim_len = env.N\n",
    "    elif action_type == 'split':\n",
    "        obs_dim = (env.neighbor_num + 1, 1 + extra_num)\n",
    "        act_dim = (2,)\n",
    "        dim_len = env.N\n",
    "        \n",
    "    checkpoint = torch.load(rel_path+f'pyt_save/model{epoch}.pth')\n",
    "    ac = core.ActorCritic(obs_dim, act_dim, arch, **ac_kwargs)\n",
    "    ac.pi.load_state_dict(checkpoint['pi'])\n",
    "    ac.v.load_state_dict(checkpoint['v'])\n",
    "\n",
    "    Parallel = DataParallel\n",
    "    parallel_args = {\n",
    "        'device_ids': list(range(1)),\n",
    "        'output_device': 0\n",
    "    } \n",
    "\n",
    "    ac.pi = Parallel(ac.pi, **parallel_args)\n",
    "    ac.v = Parallel(ac.v, **parallel_args)\n",
    "    ac.eval()\n",
    "    return ac, obs_dim, act_dim, dim_len, gamma, lam, env_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_L200 2025\n",
    "# complete_L200_2 2269\n",
    "# complete_L300 2593\n",
    "# maxmc_L100 4483\n",
    "# maxmc_L200 3761\n",
    "# 79, 177inspection_dict_SIRF_TT_gene_ent_EC0.003_N15K7NN3RSFTMT_Z_adam_cr-1_lr1e-5_g98_cp_E5400_E550\n",
    "exp_name = 'SIRF_TT_gene_ent_EC0.003_N15K7NN3RSFTMT_Z_adam_cr-1_lr1e-5_g98_cp_E5400'\n",
    "epoch = 550\n",
    "ac, obs_dim, act_dim, dim_len, gamma, lam, env_kwargs = load_model(exp_name, epoch)\n",
    "reward_supply_type = 'full'\n",
    "env_kwargs['rescale'] = False\n",
    "terminate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492dbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 32\n",
    "M = 100\n",
    "N = 15\n",
    "K = 7\n",
    "NN = 3\n",
    "exp = 8\n",
    "trj_len = 200\n",
    "graph_type = 'complete'\n",
    "reward_type = 'indv_raw_full'\n",
    "action_type = 'total'\n",
    "extra_type = 'SIRF'\n",
    "env_name = 'SL_NK_' + action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_num = 1\n",
    "index_list = [i*11 + copy_num for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_num = 6\n",
    "index_list += [i*11 + copy_num for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_num = 7\n",
    "index_list += [i*11 + copy_num for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c700389",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [51, 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6eef2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# normal test, without unique/prob\n",
    "\n",
    "for index in index_list:\n",
    "\n",
    "    scr_buf_list = []\n",
    "    final_score_list = []\n",
    "    Ret_list = []\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    graph.add_edges_from([tuple(x) for x in real_network[real_network['network_index']==network_index[index]]['edges_id'].values[0]]) # add weights to the edges\n",
    "    graph2 = nx.k_core(graph, k=3)\n",
    "    graph3 = nx.convert_node_labels_to_integers(graph2)\n",
    "\n",
    "    env_kwargs = {\n",
    "            'E': E,\n",
    "            'M': network_nodes[index],\n",
    "            'N': N,\n",
    "            'K': K,\n",
    "            'neighbor_num': NN,\n",
    "            'exp': exp,\n",
    "            'graph': nx.from_edgelist,\n",
    "            'graph_dict': {'edgelist': graph3.edges},\n",
    "            'reward_type': reward_type,\n",
    "            'action_type': action_type,\n",
    "            'extra_type': extra_type,\n",
    "        'corr_type': 'TT'\n",
    "        }\n",
    "    \n",
    "    env_num = 5\n",
    "    env_list = [envs.__dict__[env_name](**env_kwargs) for i in range(env_num)]\n",
    "    start_time = time.time()\n",
    "    for i in range(env_num):\n",
    "        print(i)\n",
    "        test_ensemble_num = 20\n",
    "        buf = PPOBuffer(\n",
    "            obs_dim, \n",
    "            act_dim, \n",
    "            test_ensemble_num, \n",
    "            env_kwargs['M'], \n",
    "            dim_len, \n",
    "            trj_len, \n",
    "            gamma, \n",
    "            lam, \n",
    "            split=True if env_kwargs['action_type'] == 'split' else False)\n",
    "\n",
    "\n",
    "        env = env_list[i]\n",
    "        o, _ = env.reset(test_ensemble_num, base=True) \n",
    "        ep_ret, ep_len = 0, 0\n",
    "        best_ep_ret = -np.inf\n",
    "\n",
    "        for t in range(trj_len):\n",
    "            epoch_ended = t == trj_len - 1\n",
    "            a, v, logp, pi = ac.step(torch.as_tensor(o, dtype=torch.float32, device='cuda'), return_pi=True)\n",
    "\n",
    "            next_o, r, s = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            if reward_supply_type == 'full':\n",
    "                buf.store(o, a, r, v, s, logp)\n",
    "            else:\n",
    "                if epoch_ended:\n",
    "                    if reward_supply_type == 'final':\n",
    "                        buf.store(o, a, r * trj_len, v, s, logp)\n",
    "                    elif reward_supply_type == 'finalmean':\n",
    "                        buf.store(o, a, ep_ret, v, s, logp)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                else:\n",
    "                    buf.store(o, a, 0, v, s, logp)\n",
    "\n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "\n",
    "            if epoch_ended:\n",
    "                a, v, logp, pi = ac.step(\n",
    "                    torch.as_tensor(o, dtype=torch.float32, device='cuda' ),\n",
    "                    return_pi=True\n",
    "                )\n",
    "                _, _, s = env.step(a)\n",
    "                if terminate:\n",
    "                    buf.finish_path(np.zeros_like(v))\n",
    "                else:\n",
    "                    buf.finish_path(v)\n",
    "\n",
    "        Ret=ep_ret / ep_len\n",
    "        Ret_list.append(Ret)\n",
    "        EpLen=ep_len\n",
    "        FinalScore=np.mean(s)\n",
    "        scr_buf_list.append(buf.scr_buf)\n",
    "        final_score_list.append(FinalScore)\n",
    "        ep_ret, ep_len = 0, 0\n",
    "\n",
    "    Ret_list = np.array(Ret_list)\n",
    "    final_score_list = np.array(final_score_list)\n",
    "    scr_buf_list = np.array(scr_buf_list)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'[Network {index}, ({network_index[index]}, M={network_nodes[index]}), (Time : {elapsed_time})]: {np.mean(Ret_list)}, {np.mean(final_score_list)}', )\n",
    "    inspection_dict = {}\n",
    "    inspection_dict['scr_buf_list'] = scr_buf_list\n",
    "    \n",
    "    with open(f'./result/real_network_{index}_RL.pkl', 'wb') as f:\n",
    "        pickle.dump(inspection_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b7327",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38dac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data_dict = {}\n",
    "baseline_data_dict['keys'] = ['Ret', 'FinalScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e3d616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d56c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 20\n",
    "M = 100\n",
    "N = 15\n",
    "K = 7\n",
    "NN = 3\n",
    "exp = 8\n",
    "trj_len = 200\n",
    "graph_type = 'complete'\n",
    "reward_type = 'indv_raw_full'\n",
    "action_type = 'total'\n",
    "extra_type = 'SIRF'\n",
    "env_name = 'SL_NK_' + action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff6cdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline construction initiated\n",
      "Baseline : FollowBest_random\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[Network 51, FollowBest_random, (190.0, M=477, (Time : 5075.557312965393)]: 0.8753731260030468, 0.9814911998464309\n",
      "Baseline : FollowBest_prob\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[Network 51, FollowBest_prob, (190.0, M=477, (Time : 5443.212260961533)]: 0.8509198078442397, 0.9034066104465281\n",
      "Baseline : FollowMajor_random\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[Network 51, FollowMajor_random, (190.0, M=477, (Time : 7211.379746198654)]: 0.3212149419370918, 0.4267530584246096\n",
      "Baseline : FollowMajor_prob\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[Network 51, FollowMajor_prob, (190.0, M=477, (Time : 7106.854659080505)]: 0.39592679118822965, 0.5617139088320205\n",
      "Baseline : IndvRandom\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[Network 51, IndvRandom, (190.0, M=477, (Time : 5015.182075738907)]: 0.3065955063715522, 0.3899396059232285\n",
      "Baseline : IndvProb\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[Network 51, IndvProb, (190.0, M=477, (Time : 4859.5202004909515)]: 0.33948635592886706, 0.42766789074166783\n"
     ]
    }
   ],
   "source": [
    "#baselines = ['FollowBest', 'FollowBest_indv', 'FollowMajor', 'FollowMajor_indv', 'IndvLearning', 'RandomCopy']\n",
    "baselines = ['FollowBest_random', 'FollowBest_prob', 'FollowMajor_random', 'FollowMajor_prob', 'IndvRandom', 'IndvProb',]\n",
    "\n",
    "for index in index_list:\n",
    "\n",
    "    scr_buf_list = []\n",
    "    final_score_list = []\n",
    "    Ret_list = []\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    graph.add_edges_from([tuple(x) for x in real_network[real_network['network_index']==network_index[index]]['edges_id'].values[0]]) # add weights to the edges\n",
    "    graph2 = nx.k_core(graph, k=3)\n",
    "    graph3 = nx.convert_node_labels_to_integers(graph2)\n",
    "\n",
    "    env_kwargs = {\n",
    "            'E': E,\n",
    "            'M': network_nodes[index],\n",
    "            'N': N,\n",
    "            'K': K,\n",
    "            'neighbor_num': NN,\n",
    "            'exp': exp,\n",
    "            'graph': nx.from_edgelist,\n",
    "            'graph_dict': {'edgelist': graph3.edges},\n",
    "            'reward_type': reward_type,\n",
    "            'action_type': action_type,\n",
    "            'extra_type': extra_type,\n",
    "        'corr_type': 'TT'\n",
    "        }\n",
    "    \n",
    "    env_num = 5\n",
    "    test_ensemble_num = 20\n",
    "    env_list = [envs.__dict__[env_name](**env_kwargs) for i in range(env_num)]\n",
    "    state_list = []\n",
    "    for i in range(env_num):\n",
    "        _, fixed_state = env_list[i].reset(E=test_ensemble_num, base=True)\n",
    "        state_list.append(deepcopy(fixed_state))\n",
    "    print(\"Baseline construction initiated\")\n",
    "    \n",
    "    baseline_data_dict = {}\n",
    "    baseline_data_dict['keys'] = ['Ret', 'FinalScore']\n",
    "\n",
    "    for baseline_name in baselines:\n",
    "        if baseline_name not in baseline_data_dict.keys():\n",
    "            print(f\"Baseline : {baseline_name}\")\n",
    "            start_time = time.time()\n",
    "            baseline_data = {}\n",
    "            baseline_data['Ret'] = []\n",
    "            baseline_data['FinalScore'] = []\n",
    "            baseline_data['scr_buf'] = []\n",
    "            baseline_data['unq_buf'] = []\n",
    "\n",
    "            for i in range(env_num):\n",
    "                print(i)\n",
    "                env_base = env_list[i]\n",
    "                ac_base = core.__dict__[baseline_name](env_base, action_type, extra_type, corr_type='TT')\n",
    "                scr_buf = np.zeros((test_ensemble_num, network_nodes[index], trj_len), dtype=np.float32)\n",
    "                unq_buf = np.zeros((test_ensemble_num, trj_len), dtype=np.float32)\n",
    "\n",
    "                o, _ = env_base.reset(states=state_list[i], state_only=True, base=True)\n",
    "                ep_ret, ep_len = 0, 0\n",
    "                for t in range(trj_len):\n",
    "                    a = ac_base.step(o)\n",
    "                    next_o, r, s = env_base.step(a)\n",
    "                    ep_ret += r\n",
    "                    ep_len += 1\n",
    "                    scr_buf[..., t] = s\n",
    "                    for e in range(test_ensemble_num):\n",
    "                        freq = np.unique(a[e], axis=0)\n",
    "                        unq_buf[e][t] = freq.shape[0]\n",
    "                    o = next_o\n",
    "\n",
    "                baseline_data['Ret'].append(np.mean(ep_ret / ep_len))\n",
    "                baseline_data['FinalScore'].append(np.mean(s))\n",
    "                baseline_data['scr_buf'].append(scr_buf)\n",
    "                baseline_data['unq_buf'].append(unq_buf)\n",
    "            baseline_data['Ret'] = np.mean(baseline_data['Ret'])\n",
    "            baseline_data['FinalScore'] = np.mean(baseline_data['FinalScore'])\n",
    "            baseline_data['scr_buf'] = np.array(baseline_data['scr_buf'])\n",
    "            baseline_data['unq_buf'] = np.array(baseline_data['unq_buf'])\n",
    "            baseline_data_dict[baseline_name] = baseline_data\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f'[Network {index}, {baseline_name}, ({network_index[index]}, M={network_nodes[index]}, (Time : {elapsed_time})]: {baseline_data_dict[baseline_name][\"Ret\"]}, {baseline_data_dict[baseline_name][\"FinalScore\"]}')\n",
    "    \n",
    "    with open(f'real_network_{index}_base2.pkl', 'wb') as f:\n",
    "        pickle.dump(baseline_data_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ce9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./result/real_network_{index}.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "for key in data.keys():\n",
    "    if key != 'keys':\n",
    "        print(key)\n",
    "        for key2 in data['keys']:\n",
    "            print(data[key][key2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
